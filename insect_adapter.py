import torch

def inspect_adapters(checkpoint_path):
    # åŠ è½½æ¨¡å‹å‚æ•°
    state_dict = torch.load(checkpoint_path, map_location='cpu')
    
    # å®šä¹‰é€‚é…å™¨æ ‡è¯†æ¨¡å¼
    LORA_KEYS = {'lora_A', 'lora_B', 'scaling'}
    DASH_KEYS = {'lora_index', 'weight_u_top', 'weight_vt_top'}
    
    # è‡ªåŠ¨è¯†åˆ«é€‚é…å™¨ç±»å‹
    adapters = {}
    for key in state_dict.keys():
        # è¯†åˆ«LoRAé€‚é…å™¨
        if any(lora_key in key for lora_key in LORA_KEYS):
            layer_name = key.rsplit('.', 1)[0]  # æå–å±‚åç§°
            adapters.setdefault(layer_name, {'type': []})['type'].append('LoRA')
            
        # è¯†åˆ«DASHé€‚é…å™¨
        if any(dash_key in key for dash_key in DASH_KEYS):
            layer_name = key.rsplit('.', 1)[0]
            adapters.setdefault(layer_name, {'type': []})['type'].append('DASH')
            
    # æ•´ç†æ˜¾ç¤ºç»“æœ
    print("ğŸ” å‘ç°ä»¥ä¸‹é€‚é…å™¨ç»“æ„ï¼š")
    for layer, info in adapters.items():
        types = list(set(info['type']))  # å»é‡
        num_params = sum(1 for k in state_dict if k.startswith(layer))
        
        # æ˜¾ç¤ºé€‚é…å™¨è¯¦ç»†ä¿¡æ¯
        print(f"â”œâ”€â”€ å±‚: {layer}")
        print(f"â”‚   â”œâ”€â”€ ç±»å‹: {', '.join(types)}")
        print(f"â”‚   â”œâ”€â”€ å‚æ•°é‡: {num_params}ä¸ª")
        print(f"â”‚   â””â”€â”€ å…·ä½“å‚æ•°: ")
        for param in [k for k in state_dict if k.startswith(layer)]:
            shape = tuple(state_dict[param].shape)
            print(f"â”‚       â”œâ”€â”€ {param}: {shape}")

if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    inspect_adapters("your_model_checkpoint.pth")